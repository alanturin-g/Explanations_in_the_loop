{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8c29a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC \n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from math import sqrt\n",
    "\n",
    "def get_train_test(dataset_name, path, r_seed):\n",
    "    if dataset_name == \"HeartRisk\":\n",
    "        print(\"Loading Heart\")\n",
    "        print(\"------------------------------------------------------------\")\n",
    "        heart_train_risk_path = path + \"Dataset-Merged.xlsx\"\n",
    "        dataset = pd.read_excel(heart_train_risk_path).drop(columns = [\"P_ID\"])\n",
    "        \n",
    "        \n",
    "        target = \"HeartRisk\"\n",
    "        binary = ['male', 'smoker?', 'BPMeds', 'prevalentStroke', \n",
    "                        'prevalentHyp', 'diabetes']\n",
    "        categoricals = None\n",
    "        fairness = [\"male\"]\n",
    "        \n",
    "        dataset = dataset.dropna()\n",
    "        \n",
    "        miss_values_features = [\"edu\", \"cigsPerDay\", \"BPMeds\", \"totChol\", \"glucose\"]\n",
    "        for f in miss_values_features:\n",
    "            dataset = dataset.drop(dataset[dataset[f] == -1].index)\n",
    "\n",
    "        test_size = 0.2\n",
    "    elif dataset_name == \"cancer\":\n",
    "        print(\"Loading Cancer\")\n",
    "        print(\"------------------------------------------------------------\")\n",
    "        cancer_path = path + \"wdbc2.csv\"\n",
    "        dataset = pd.read_csv(cancer_path, sep=',').drop(columns = [\"ID\"])\n",
    "        target = \"cancer type\"\n",
    "        categoricals = None\n",
    "        fairness = None\n",
    "        binary = None\n",
    "        test_size = 0.2\n",
    "    elif dataset_name == \"adult\":\n",
    "        print(\"Loading Adult\")\n",
    "        print(\"------------------------------------------------------------\")\n",
    "        adult_path = path + \"adult.csv\"\n",
    "        dataset = pd.read_csv(adult_path, sep=',').drop(columns = [\"ID\"])\n",
    "        target = \"class\"\n",
    "        categoricals = ['workclass', 'education', 'marital-status', 'occupation',\n",
    "                        'relationship', 'race', 'sex', 'native-country']\n",
    "        fairness = [\"sex\"]\n",
    "        dataset = dataset.dropna()\n",
    "        test_size = 0.2\n",
    "    elif dataset_name == \"kidney\":\n",
    "        print(\"Loading Kidney\")\n",
    "        print(\"------------------------------------------------------------\")\n",
    "        kidney_path = path + \"kidney.csv\"\n",
    "        dataset = pd.read_csv(kidney_path, sep=',')\n",
    "        target = \"class\"\n",
    "        categoricals = [\"sg\", \"al\", \"su\"]\n",
    "        fairness = None\n",
    "        binary = [\"rbc\", \"pc\", \"pcc\", \"ba\", \"htn\", \"dm\", \"cad\", \"appet\", \"pe\", \"ane\"]\n",
    "        dataset = dataset.dropna()\n",
    "        for f in dataset.columns:\n",
    "            dataset = dataset.drop(dataset[dataset[f] == '?'].index)\n",
    "        test_size = 0.4\n",
    "    elif dataset_name == \"credit\":\n",
    "        credit_card_path = path + \"credit.csv\"\n",
    "        dataset = pd.read_csv(credit_card_path, sep = ',')\n",
    "        target = \"default payment next month\"\n",
    "        categoricals = [\"EDUCATION\", \"MARRIAGE\", \"PAY_0\", \"PAY_2\", \"PAY_3\"\n",
    "                        , \"PAY_4\", \"PAY_5\", \"PAY_6\"]\n",
    "        fairness = [\"SEX\"]\n",
    "        binary = [\"SEX\"]\n",
    "        \n",
    "        test_size = 0.2\n",
    "    elif dataset_name == \"student\":\n",
    "        print(\"Loading Student\")\n",
    "        print(\"------------------------------------------------------------\")\n",
    "        student_path = path + \"student.csv\"\n",
    "        dataset = pd.read_csv(student_path, sep = ',')\n",
    "        target = \"class\"\n",
    "        categoricals = [\"Mjob\", \"Fjob\", \"reason\", \"guardian\"]\n",
    "        fairness = [\"sex\"]\n",
    "        binary = [\"school\", \"sex\",\"address\", \"famsize\", \"Pstatus\", \"schoolsup\", \"famsup\", \n",
    "                  \"paid\", \"activities\", \"nursery\", \"higher\", \"internet\", \"romantic\"]\n",
    "        \n",
    "        test_size = 0.2\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"DATASET INESISTENTE\")\n",
    "    if categoricals != None:\n",
    "        for f in categoricals:\n",
    "            one_hot = pd.get_dummies(dataset[f], \n",
    "                                     prefix = f)\n",
    "            dataset = dataset.drop(columns = [f])\n",
    "            dataset = dataset.join(one_hot)\n",
    "    \n",
    "        dataset = dataset[[c for c in dataset if c not in [target]] + [target]]\n",
    "    train_set, test_set = train_test_split(dataset, test_size = test_size, random_state = r_seed, \n",
    "                                           stratify = dataset[target])\n",
    "        \n",
    "    return train_set, test_set, list(train_set.columns[:-1]), categoricals, binary, fairness\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(dataset, train_samples, train_labels, test_samples, test_labels, r_seed, \n",
    "               columns, categoricals, binary, fairness):\n",
    "    if categoricals != None:\n",
    "        categoricals_idx = [columns.index(c2) for c1 in categoricals for c2 in columns if c2.startswith(c1+\"_\")]\n",
    "    else:\n",
    "        categoricals_idx = []\n",
    "        \n",
    "    if binary != None:\n",
    "        binary_idx = [columns.index(c1) for c1 in binary for c2 in columns if c2 == c1]\n",
    "    else:\n",
    "        binary_idx = []\n",
    "      \n",
    "    num_features_idx = list(set(range(len(columns)))-set(categoricals_idx)-set(binary_idx))\n",
    "    \n",
    "    if fairness != None:\n",
    "        fairness_idx = [columns.index(c1) for c1 in fairness for c2 in columns if c2 == c1]\n",
    "    else:\n",
    "        fairness_idx = []\n",
    "\n",
    "    std = StandardScaler()\n",
    "    std.fit(train_samples[:,num_features_idx])\n",
    "    train_samples[:,num_features_idx] = std.transform(train_samples[:,num_features_idx])\n",
    "    test_samples[:,num_features_idx] = std.transform(test_samples[:,num_features_idx])\n",
    "    \n",
    "    if categoricals != None or binary != None:\n",
    "        rescale_idx = categoricals_idx + binary_idx\n",
    "        #print(rescale_idx)\n",
    "        train_samples[:,rescale_idx] = np.where(train_samples[:,rescale_idx] == 0, -1, train_samples[:,rescale_idx])\n",
    "        test_samples[:,rescale_idx] = np.where(test_samples[:,rescale_idx] == 0, -1, test_samples[:,rescale_idx])\n",
    "        sm = SMOTENC(categorical_features = rescale_idx, random_state = r_seed)\n",
    "    else:\n",
    "        sm = SMOTE(random_state = r_seed)\n",
    "    train_samples, train_labels = sm.fit_resample(train_samples, train_labels)\n",
    "    test_samples, test_labels = sm.fit_resample(test_samples, test_labels)\n",
    "    \n",
    "    if fairness != None:\n",
    "        samples = np.concatenate((train_samples, test_samples))\n",
    "        males = [i for i in range(samples.shape[0]) if samples[i,fairness_idx] == 1]\n",
    "        females = [i for i in range(samples.shape[0]) if samples[i,fairness_idx] == -1]\n",
    "    else:\n",
    "        males = None\n",
    "        females = None\n",
    "        \n",
    "    \n",
    "    return train_samples, train_labels, test_samples, test_labels, males, females"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1242e6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class EarlyStoppingAtMinLoss(keras.callbacks.Callback):\n",
    "    def __init__(self, test_samples, test_labels, patience = 2, min_iterations = 10):\n",
    "        super(EarlyStoppingAtMinLoss, self).__init__()\n",
    "        self.min_iterations = min_iterations\n",
    "        self.patience = patience\n",
    "        self.samples = test_samples\n",
    "        self.labels = test_labels\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best = np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = self.model.evaluate(self.samples, self.labels, verbose = 0)[0]\n",
    "        if np.less(current, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            # Record the best weights if current results is better (less).\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        elif epoch >= self.min_iterations:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "                \n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))\n",
    "\n",
    "def compute_model(train_samples, train_labels, test_samples, test_labels, file_path, model_layers, \n",
    "                  patience = 2, min_iterations = 10, regularizer = None):\n",
    "    input_dim = train_samples.shape[1]\n",
    "    if regularizer == \"l1\":\n",
    "        regularizer = tf.keras.regularizers.l1(l1 = 0.0001)\n",
    "    elif regularizer == \"l2\":\n",
    "        regularizer = tf.keras.regularizers.l2(l2 = 0.0001)\n",
    "    elif regularizer == \"l1_l2\":\n",
    "        regularizer = tf.keras.regularizers.l1_l2(l1 = 0.0001, l2 = 0.0001)\n",
    "    n_class = 1\n",
    "    if(not os.path.isdir(file_path)):\n",
    "        if model_layers == 1:\n",
    "            model = Sequential([Dense(units = input_dim, input_shape= (input_dim,),\n",
    "                                      activation = \"relu\", kernel_regularizer=regularizer),\n",
    "                Dense(units = n_class, activation = \"sigmoid\")])\n",
    "        else:\n",
    "            model = Sequential([Dense(units = input_dim, input_shape= (input_dim,), \n",
    "                                      activation = \"relu\", kernel_regularizer=regularizer),\n",
    "                Dense(units = input_dim/2, activation = \"relu\", kernel_regularizer=regularizer),\n",
    "                Dense(units = n_class, activation = \"sigmoid\")])\n",
    "        early_stopping = EarlyStoppingAtMinLoss(test_samples, test_labels, patience, min_iterations)\n",
    "        callbacks = [early_stopping]\n",
    "            \n",
    "        model.compile(optimizer = Adam(learning_rate = 0.001), \n",
    "                      loss = \"binary_crossentropy\", metrics = [\"binary_accuracy\"])\n",
    "        \n",
    "        history = model.fit(x = train_samples, y = train_labels, batch_size = 10, \n",
    "                            epochs = 1000, verbose = 1, callbacks = callbacks) \n",
    "        \n",
    "        np.save(file_path+\".npy\", history.history)\n",
    "        \n",
    "        model.save(file_path)\n",
    "    \n",
    "def evaluate_model(model, train_samples, train_labels, test_samples, test_labels):  \n",
    "    print(\"Evaluate on train data\")\n",
    "    print(\"TRAIN: \")\n",
    "    train_pred = (model.predict(train_samples) > 0.5).astype(\"int32\")\n",
    "    dict_train = classification_report(train_labels, train_pred, output_dict = True)\n",
    "    fpr, tpr, thresholds = roc_curve(train_labels, train_pred)\n",
    "    auc_train = auc(fpr, tpr)\n",
    "    print(\"AUC: \", auc(fpr, tpr))\n",
    "    print(\"___________________________________________________________\")\n",
    "    \n",
    "    print(\"Evaluate on test data\")\n",
    "    print(\"TEST: \")\n",
    "    test_pred = (model.predict(test_samples) > 0.5).astype(\"int32\")\n",
    "    dict_test = classification_report(test_labels, test_pred, output_dict = True)\n",
    "    preds = model.predict(test_samples)\n",
    "    fpr, tpr, thresholds = roc_curve(test_labels, test_pred)\n",
    "    auc_test = auc(fpr, tpr)\n",
    "    print(\"AUC: \", auc(fpr, tpr))\n",
    "    print(\"___________________________________________________________\")\n",
    "    return dict_train, dict_test, auc_train, auc_test\n",
    "    \n",
    "def confusion_matrix(model, samples, labels):\n",
    "    predictions = (model.predict(samples) > 0.5).astype(\"int32\")\n",
    "    tp = []\n",
    "    tn = []\n",
    "    fp = []\n",
    "    fn = []\n",
    "    for i in range(samples.shape[0]):\n",
    "        if predictions[i] == 1 and labels[i] == 1:\n",
    "            tp.append(i)\n",
    "        elif predictions[i] == 0 and labels[i] == 0:\n",
    "            tn.append(i)\n",
    "        elif predictions[i] == 0 and labels[i] == 1:\n",
    "            fn.append(i)\n",
    "        elif predictions[i] == 1 and labels[i] == 0:\n",
    "            fp.append(i)\n",
    "    tp = np.asarray(tp)\n",
    "    tn = np.asarray(tn)\n",
    "    fp = np.asarray(fp)\n",
    "    fn = np.asarray(fn)\n",
    "    print(len(tp), len(tn), len(fp), len(fn))\n",
    "    print(\"_____________________________________________________________\")\n",
    "    return tp, tn, fp, fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d69e52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statistics\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def compute_shap_values(file_path, samples, columns, explainer):\n",
    "    if(not os.path.isfile(file_path + \".tsv\")):\n",
    "            print(\"START CALCULATING SHAP VALUES...\")\n",
    "            shap_values = explainer.shap_values(samples, silent = True)\n",
    "            print(\"END CALCULATING SHAP VALUES\")\n",
    "            pd.DataFrame(shap_values[0]).to_csv(file_path + \".tsv\", sep = \"\\t\",\n",
    "                                                index = False)\n",
    "            \n",
    "            shap.summary_plot(shap_values, samples, feature_names = columns, show=False)\n",
    "            plt.savefig(file_path + \".png\")\n",
    "            plt.clf()\n",
    "            print(\"_____________________________________________________________________\")\n",
    "\n",
    "def get_shap_values(file_path, model, samples):\n",
    "    shap_values_as_df = pd.read_csv(file_path + \".tsv\", sep='\\t')\n",
    "    \n",
    "    return np.asarray(shap_values_as_df).astype(\"float\")\n",
    "         \n",
    "\n",
    "def weight_samples(shap_values, samples, scaler):\n",
    "    return np.multiply(samples,np.abs(shap_values)) / scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbad4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Cancer\n",
      "------------------------------------------------------------\n",
      "Evaluate on train data\n",
      "TRAIN: \n",
      "AUC:  0.9947368421052631\n",
      "___________________________________________________________\n",
      "Evaluate on test data\n",
      "TEST: \n",
      "AUC:  0.9861111111111112\n",
      "___________________________________________________________\n",
      "353 356 1 4\n",
      "_____________________________________________________________\n",
      "0_median_model_1_hidden\n",
      "Epoch 1/1000\n",
      "57/57 [==============================] - 1s 4ms/step - loss: 0.3617 - binary_accuracy: 0.9333\n",
      "Epoch 2/1000\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.1648 - binary_accuracy: 0.9772\n",
      "Epoch 3/1000\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.1024 - binary_accuracy: 0.9807\n",
      "Epoch 4/1000\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.0758 - binary_accuracy: 0.9842\n",
      "Epoch 5/1000\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.0617 - binary_accuracy: 0.9842\n",
      "Epoch 6/1000\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.0523 - binary_accuracy: 0.9860\n",
      "Epoch 7/1000\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.0461 - binary_accuracy: 0.9860\n",
      "Epoch 8/1000\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.0405 - binary_accuracy: 0.9860\n",
      "Epoch 9/1000\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.0367 - binary_accuracy: 0.9860\n",
      "Epoch 10/1000\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.0326 - binary_accuracy: 0.9860\n",
      "Epoch 11/1000\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.0294 - binary_accuracy: 0.9877\n",
      "Epoch 12/1000\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.0267 - binary_accuracy: 0.9912\n",
      "Epoch 13/1000\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.0240 - binary_accuracy: 0.9912\n",
      "Epoch 14/1000\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.0220 - binary_accuracy: 0.9912\n",
      "Epoch 15/1000\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.0202 - binary_accuracy: 0.9912\n",
      "Epoch 16/1000\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.0182 - binary_accuracy: 0.9930\n",
      "Epoch 17/1000\n",
      " 1/57 [..............................] - ETA: 0s - loss: 5.8275e-04 - binary_accuracy: 1.0000Restoring model weights from the end of the best epoch.\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 0.0168 - binary_accuracy: 0.9947\n",
      "Epoch 00017: early stopping\n",
      "INFO:tensorflow:Assets written to: results IDW/cancer/1 hidden layer/1_median_model_1_hidden\\assets\n",
      "Evaluate on train data\n",
      "TRAIN: \n",
      "AUC:  0.9912280701754386\n",
      "___________________________________________________________\n",
      "Evaluate on test data\n",
      "TEST: \n",
      "AUC:  0.9861111111111112\n",
      "___________________________________________________________\n",
      "352 355 2 5\n",
      "_____________________________________________________________\n",
      "START CALCULATING SHAP VALUES...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from numpy.random import seed\n",
    "import statistics\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def save_results(dict_train, dict_test, zeros_train, zeros_test, auc_train, auc_test, file_path, title):\n",
    "    with open(\"/\".join(file_path.split(\"/\")[:-2])+ \"/results \" + \n",
    "              title + \".tsv\", \"a\") as res_file:\n",
    "        tsv_writer = csv.writer(res_file, delimiter = \"\\t\")\n",
    "        print(file_path.split(\"/\")[-1])\n",
    "        tsv_writer.writerow([file_path.split(\"/\")[-1] + \" train\",\"precision\",\"recall\", \"f1\", \"zeros shap\", \"AUC\", \"Accuracy\"])\n",
    "        tsv_writer.writerow([0, dict_train[\"0\"][\"precision\"], dict_train[\"0\"][\"recall\"], dict_train[\"0\"][\"f1-score\"]])\n",
    "        tsv_writer.writerow([1, dict_train[\"1\"][\"precision\"], dict_train[\"1\"][\"recall\"], dict_train[\"1\"][\"f1-score\"]])\n",
    "        tsv_writer.writerow([\"macro\", dict_train[\"macro avg\"][\"precision\"], \n",
    "                             dict_train[\"macro avg\"][\"recall\"], dict_train[\"macro avg\"][\"f1-score\"], zeros_train, auc_train,\n",
    "                            dict_train[\"accuracy\"]])\n",
    "        \n",
    "        tsv_writer.writerow([file_path.split(\"/\")[-1] + \" test\",\"precision\",\"recall\", \"f1\"])\n",
    "        tsv_writer.writerow([0, dict_test[\"0\"][\"precision\"], dict_test[\"0\"][\"recall\"], dict_test[\"0\"][\"f1-score\"]])\n",
    "        tsv_writer.writerow([1, dict_test[\"1\"][\"precision\"], dict_test[\"1\"][\"recall\"], dict_test[\"1\"][\"f1-score\"]])\n",
    "        tsv_writer.writerow([\"macro\", dict_test[\"macro avg\"][\"precision\"], \n",
    "                             dict_test[\"macro avg\"][\"recall\"], dict_test[\"macro avg\"][\"f1-score\"], zeros_test, auc_test,\n",
    "                            dict_test[\"accuracy\"]])\n",
    "    res_file.close()\n",
    "                        \n",
    "def save_confusion_matrix(tps, tns, fps, fns, title):\n",
    "    with open(title + \".tsv\", \"w\") as cf_file:\n",
    "        tsv_writer = csv.writer(cf_file, delimiter = \"\\t\")\n",
    "        for i in range(len(tps)):\n",
    "            tsv_writer.writerow([\"Iter \" + str(i) + \" TPs\"])\n",
    "            tsv_writer.writerow(tps[i])\n",
    "            tsv_writer.writerow([\"Iter \" + str(i) + \" TNs\"])\n",
    "            tsv_writer.writerow(tns[i])\n",
    "            tsv_writer.writerow([\"Iter \" + str(i) + \" FPs\"])\n",
    "            tsv_writer.writerow(fps[i])\n",
    "            tsv_writer.writerow([\"Iter \" + str(i) + \" FNs\"])\n",
    "            tsv_writer.writerow(fns[i])\n",
    "    cf_file.close()\n",
    "\n",
    "def count_zeros(data):\n",
    "    return np.sum([np.count_nonzero(np.abs(x) < 0.01) for x in data])\n",
    "\n",
    "    \n",
    "r_seed = 1\n",
    "\n",
    "seed(r_seed)\n",
    "tf.random.set_seed(r_seed)\n",
    "\"\"\"\n",
    "Method that runs IDW method:\n",
    "    - dataset: Name of the chosen dataset. {\"cancer\", \"HeartRisk\", \"student\", \"kidney\"}\n",
    "    - data_path: Path to the dataset folder\n",
    "    - results_path: Path where to save the results\n",
    "    - regularizer: Type of regularizer. {None, \"l1\", \"l2\", \"l1_l2\"}\n",
    "    - title: title to append at the files names\n",
    "\n",
    "Results saved by the method:\n",
    "    - keras models trained during IDW: {n_iter}_median_model_1_hidden\n",
    "    - history of the keras models: {n_iter}_median_model_1_hidden.npy\n",
    "    - SHAP scores computed on the train set: {n_iter}_median_model_1_hidden.tsv and {n_iter}_median_model_1_hidden.png\n",
    "    - SHAP scores computed on the test set: {n_iter}_median_model_1_hidden_test.tsv and {n_iter}_median_model_1_hidden_test.png\n",
    "    - precision, recall, f1, XCP, AUC and accuracy: results {title}.tsv\n",
    "    - confusion matrixs: confusion matrix {title}.tsv\n",
    "\"\"\"\n",
    "def main(dataset, data_path, results_path, title, regularizer = None):\n",
    "    model_layers = 1\n",
    "    \n",
    "    train_set, test_set, columns, categoricals, binary, fairness = get_train_test(dataset, data_path, r_seed)\n",
    "        \n",
    "    if categoricals != None:\n",
    "        categoricals_idx = [columns.index(c2) for c1 in categoricals for c2 in columns if c2.startswith(c1+\"_\")]\n",
    "    else:\n",
    "        categoricals_idx = []\n",
    "      \n",
    "    if binary != None:\n",
    "        binary_idx = [columns.index(c1) for c1 in binary for c2 in columns if c2 == c1]\n",
    "    else:\n",
    "        binary_idx = []\n",
    "      \n",
    "    num_features_idx = list(set(range(len(columns)))-set(categoricals_idx)-set(binary_idx))\n",
    "    \n",
    "    if fairness != None:\n",
    "        fairness_idx = [columns.index(c1) for c1 in fairness for c2 in columns if c2 == c1]\n",
    "    else:\n",
    "        fairness_idx = []\n",
    "        \n",
    "    train_samples = np.asarray(train_set[train_set.columns[:-1]]).astype(\"float\")\n",
    "    train_labels = np.asarray(train_set[train_set.columns[-1]]).astype(\"int64\")\n",
    "    \n",
    "     \n",
    "    test_samples = np.asarray(test_set[test_set.columns[:-1]]).astype(\"float\")\n",
    "    test_labels = np.asarray(test_set[test_set.columns[-1]]).astype(\"int64\")\n",
    "\n",
    "    train_samples, train_labels, test_samples, test_labels, males, females = \\\n",
    "                preprocess(dataset, train_samples, \n",
    "                    train_labels, test_samples, \n",
    "                    test_labels, r_seed, columns,\n",
    "                    categoricals, binary, fairness)\n",
    "    \n",
    "    tps = []\n",
    "    tns = []\n",
    "    fps = []\n",
    "    fns = []\n",
    "    new_train = train_samples\n",
    "    new_test = test_samples\n",
    "    times = []\n",
    "    epochs = []\n",
    "    \n",
    "    if regularizer == None:\n",
    "        n_iter = 5\n",
    "    else:\n",
    "        n_iter = 1\n",
    "    for i in range(n_iter):\n",
    "        if model_layers == 1:\n",
    "            file_path = results_path + str(i) + \"_median_model_1_hidden\"\n",
    "                \n",
    "        \n",
    "        start_time = time.time()\n",
    "        compute_model(new_train, train_labels, new_test, \n",
    "                      test_labels, file_path, model_layers, patience = 5, \n",
    "                      min_iterations = 10, regularizer = regularizer)\n",
    "        \n",
    "        model = keras.models.load_model(file_path)\n",
    "        history=np.load(file_path+ '.npy',allow_pickle='TRUE').item()\n",
    "        \n",
    "        dict_train, dict_test, auc_train, auc_test = evaluate_model(model, new_train, \n",
    "                                               train_labels, new_test, test_labels)\n",
    "        \n",
    "        \n",
    "        samples = new_train\n",
    "        if males != None and females != None:\n",
    "            \n",
    "            males = [e for e in males if e < samples.shape[0]]\n",
    "            females = [e for e in females if e < samples.shape[0]]\n",
    "            \n",
    "            m = samples[males]\n",
    "            f = samples[females]\n",
    "            \n",
    "            background_sample_m = shap.kmeans(m, 2)\n",
    "            background_sample_f = shap.kmeans(f, 2)\n",
    "            background_sample = np.concatenate((background_sample_m.data, background_sample_f.data))\n",
    "\n",
    "        else:\n",
    "            background_sample = shap.kmeans(samples, 4).data\n",
    "            \n",
    "\n",
    "        samples = np.concatenate((new_train, new_test))\n",
    "        labels = np.concatenate((train_labels, test_labels))\n",
    "        tp, tn, fp, fn = confusion_matrix(model, samples, labels)\n",
    "        \n",
    "        explainer = shap.KernelExplainer(model.predict, background_sample)\n",
    "                \n",
    "        tps.append(tp)\n",
    "        tns.append(tn)\n",
    "        fps.append(fp)\n",
    "        fns.append(fn)\n",
    "        \n",
    "        shap_path = file_path\n",
    "        compute_shap_values(shap_path, new_train, train_set.columns[:-1], explainer)\n",
    "        shap_values_train = get_shap_values(shap_path, model, new_train)\n",
    "        zeros_train = count_zeros(shap_values_train)\n",
    "        zeros_train = zeros_train/(new_train.shape[0] * new_train.shape[1])\n",
    "\n",
    "        shap_path = file_path + \"_test\"\n",
    "        compute_shap_values(shap_path, new_test, test_set.columns[:-1], explainer)\n",
    "        shap_values_test = get_shap_values(shap_path, model, new_test)\n",
    "        zeros_test = count_zeros(shap_values_test)\n",
    "        zeros_test = zeros_test/(new_test.shape[0] * new_test.shape[1])\n",
    "\n",
    "        new_train = weight_samples(shap_values_train, new_train, 1)\n",
    "        new_test =  weight_samples(shap_values_test, new_test, 1)\n",
    "        \n",
    "        std = StandardScaler()\n",
    "        std.fit(new_train)\n",
    "        new_train = std.transform(new_train)\n",
    "        new_test = std.transform(new_test)\n",
    "        \n",
    "        save_results(dict_train, dict_test, zeros_train, zeros_test, auc_train, auc_test, file_path, title)\n",
    "\n",
    "        time_ = time.time() - start_time\n",
    "        times.append(time_)\n",
    "        epochs.append(len(history['loss']))\n",
    "            \n",
    "    title_ = \"/\".join(file_path.split(\"/\")[:-2]) + \"/confusion matrix \" + title\n",
    "    save_confusion_matrix(tps,tns,fps,fns,title_)\n",
    "    print(epochs, times)\n",
    "\n",
    "\n",
    "\n",
    "data_path = \"dataset/cancer/\" \n",
    "\n",
    "results_path = \"results IDW/cancer/1 hidden layer/\"\n",
    "title = \"cancer 1 hidden\"\n",
    "main(\"cancer\", data_path, results_path, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a319f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"cancer\"\n",
    "\n",
    "labels = [\"Baseline\", \"Iter. 1\", \"Iter. 2\", \"Iter. 3\", \"Iter. 4\"]\n",
    "\n",
    "accs = []\n",
    "\n",
    "path =  \"results IDW/\"\n",
    "file = open(path + dataset + \"/\" + \"results \" + dataset +  \" 1 hidden.tsv\")\n",
    "res = pd.read_csv(file, sep='\\t')\n",
    "\n",
    "accs.append(res[res[res.columns[0]] == \"macro\"][\"Accuracy\"].to_numpy(dtype=\"float64\"))\n",
    "accs = np.array(accs).flatten()   \n",
    "\n",
    "accs_train = accs[0::2]\n",
    "accs_test = accs[1::2]\n",
    "\n",
    "steps = len(labels)\n",
    "\n",
    "f, ax = plt.subplots(1,1, figsize = (6,6))\n",
    "\n",
    "ax.set_ylim(0.5, 1.01)\n",
    "ax.set_yticks(np.arange(0.5,1.05,0.1))\n",
    "ax.set_yticklabels(labels = np.round(np.arange(0.51,1.05,0.1),1), fontsize = 20)\n",
    "ax.set_xticks(np.arange(0,steps,1))\n",
    "ax.set_xticklabels(labels = labels, rotation = 90, fontsize = 20) \n",
    "ax.plot(accs_test[:5], color = 'r', alpha = 0.7)\n",
    "ax.set_ylabel(\"Accuracy\", fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201adb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "xcps = []\n",
    "for i in range(5):\n",
    "    path = \"results IDW\"+\"/\"+dataset+\"/\"+\"1 hidden layer/\"+str(i)+\"_median_model_1_hidden_test\"\n",
    "\n",
    "    shap_train = get_shap_values(path, None, None)\n",
    "\n",
    "    den = shap_train.shape[0] * shap_train.shape[1]\n",
    "    xcps.append(np.count_nonzero(np.abs(shap_train) < 0.01)/den)\n",
    "f, ax = plt.subplots(1,1, figsize = (6,6))\n",
    "\n",
    "steps = len(labels)\n",
    "\n",
    "ax.plot(xcps, lw = 2, alpha = 0.7)\n",
    "ax.set_yticklabels(labels = np.round(np.arange(0.51,1.05,0.1),1), fontsize = 20)\n",
    "ax.set_xticks(np.arange(0,steps,1))\n",
    "ax.set_xticklabels(labels = labels, rotation = 90, fontsize = 18) \n",
    "ax.set_ylabel(\"Explanation Compactness Percentage\", fontsize = 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91ddee8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
